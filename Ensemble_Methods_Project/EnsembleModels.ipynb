{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8gU7AYPXMmA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## About iPython Notebooks ##\n",
    "\n",
    "iPython Notebooks are interactive coding environments embedded in a webpage. You will be using iPython notebooks in this class. Make sure you fill in any place that says `# BEGIN CODE HERE #END CODE HERE`. After writing your code, you can run the cell by either pressing \"SHIFT\"+\"ENTER\" or by clicking on \"Run\" (denoted by a play symbol). Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). \n",
    "\n",
    " **What you need to remember:**\n",
    "\n",
    "- Run your cells using SHIFT+ENTER (or \"Run cell\")\n",
    "- Write code in the designated areas using Python 3 only\n",
    "- Do not modify the code outside of the designated areas\n",
    "- In some cases you will also need to explain the results. There will also be designated areas for that. \n",
    "\n",
    "Fill in your **NAME** and **AEM** below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lO-jJrtNXMmH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NAME = \"Αλέξανδρος Τσίγγος\"\n",
    "AEM = \"3690\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sh0EE7BJXMmJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_VpnGyWXMmK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 3 - Ensemble Methods #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dQ9XoGQXMmK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Welcome to your third assignment. This exercise will test your understanding on Ensemble Methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JvHYIhS-XMmL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Always run this cell\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# USE THE FOLLOWING RANDOM STATE FOR YOUR CODE\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Always run this cell\n",
    "# Στο κελί αυτό συμπεριλαμβάνονται όλα τα imports που θα χρειαστούν στο notebook\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joKwpih2XMmM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Download the Dataset ##\n",
    "Download the dataset using the following cell or from this [link](https://github.com/sakrifor/public/tree/master/machine_learning_course/EnsembleDataset) and put the files in the same folder as the .ipynb file. \n",
    "In this assignment you are going to work with a dataset originated from the [ImageCLEFmed: The Medical Task 2016](https://www.imageclef.org/2016/medical) and the **Compound figure detection** subtask. The goal of this subtask is to identify whether a figure is a compound figure (one image consists of more than one figure) or not. The train dataset consits of 4197 examples/figures and each figure has 4096 features which were extracted using a deep neural network. The *CLASS* column represents the class of each example where 1 is a compoung figure and 0 is not. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJdwPr0bXMmM",
    "outputId": "70f064fa-7ee5-4985-f59d-9c94c1d1c3f2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('test_set_noclass.csv', <http.client.HTTPMessage at 0x27a65194d48>)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url_train = 'https://github.com/sakrifor/public/raw/master/machine_learning_course/EnsembleDataset/train_set.csv'\n",
    "filename_train = 'train_set.csv'\n",
    "urllib.request.urlretrieve(url_train, filename_train)\n",
    "url_test = 'https://github.com/sakrifor/public/raw/master/machine_learning_course/EnsembleDataset/test_set_noclass.csv'\n",
    "filename_test = 'test_set_noclass.csv'\n",
    "urllib.request.urlretrieve(url_test, filename_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "t0OVtYr7XMmN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to load the data\n",
    "train_set = pd.read_csv(\"train_set.csv\").sample(frac=1).reset_index(drop=True)\n",
    "train_set.head()\n",
    "X = train_set.drop(columns=['CLASS'])\n",
    "y = train_set['CLASS'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxOGHSmqXMmO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.0 Testing different ensemble methods ##\n",
    "In this part of the assignment you are asked to create and test different ensemble methods using the train_set.csv dataset. You should use **10-fold cross validation** for your tests and report the average f-measure weighted and balanced accuracy of your models. You can use [cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate) and select both metrics to be measured during the evaluation. Otherwise, you can use [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold).\n",
    "\n",
    "### !!! Use n_jobs=-1 where is posibble to use all the cores of a machine for running your tests ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ww_u4OlrXMmO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.1 Voting ###\n",
    "Create a voting classifier which uses three **simple** estimators/classifiers. Test both soft and hard voting and choose the best one. Consider as simple estimators the following:\n",
    "\n",
    "\n",
    "*   Decision Trees\n",
    "*   Linear Models\n",
    "*   Probabilistic Models (Naive Bayes)\n",
    "*   KNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RwvPacgkXMmP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_19088\\4091424239.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[0mhard_vcls\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mVotingClassifier\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mestimators\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'DTC'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcls1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;34m'LR'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcls2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;34m'KNN'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcls3\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvoting\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'hard'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# Voting Classifier\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 15\u001B[1;33m \u001B[0msvlcs_scores\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcross_validate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msoft_vcls\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscoring\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;34m'f1_weighted'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'balanced_accuracy'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcv\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn_jobs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     16\u001B[0m \u001B[0ms_avg_fmeasure\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msvlcs_scores\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'test_f1_weighted'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msvlcs_scores\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'test_f1_weighted'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# The average f-measure\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[0ms_avg_accuracy\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msvlcs_scores\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'test_balanced_accuracy'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msvlcs_scores\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'test_balanced_accuracy'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# The average accuracy\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\MLenv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36minner_f\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     70\u001B[0m                           FutureWarning)\n\u001B[0;32m     71\u001B[0m         \u001B[0mkwargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m{\u001B[0m\u001B[0mk\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0marg\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mk\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 72\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     73\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0minner_f\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\MLenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001B[0m in \u001B[0;36mcross_validate\u001B[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001B[0m\n\u001B[0;32m    246\u001B[0m             \u001B[0mreturn_times\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreturn_estimator\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mreturn_estimator\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    247\u001B[0m             error_score=error_score)\n\u001B[1;32m--> 248\u001B[1;33m         for train, test in cv.split(X, y, groups))\n\u001B[0m\u001B[0;32m    249\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    250\u001B[0m     \u001B[0mzipped_scores\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mscores\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\MLenv\\lib\\site-packages\\joblib\\parallel.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1059\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1060\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mretrieval_context\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1061\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mretrieve\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1062\u001B[0m             \u001B[1;31m# Make sure that we get a last message telling us we are done\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1063\u001B[0m             \u001B[0melapsed_time\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_start_time\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\MLenv\\lib\\site-packages\\joblib\\parallel.py\u001B[0m in \u001B[0;36mretrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    938\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    939\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'supports_timeout'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 940\u001B[1;33m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_output\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mjob\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    941\u001B[0m                 \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    942\u001B[0m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_output\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mjob\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\MLenv\\lib\\site-packages\\joblib\\_parallel_backends.py\u001B[0m in \u001B[0;36mwrap_future_result\u001B[1;34m(future, timeout)\u001B[0m\n\u001B[0;32m    540\u001B[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001B[0;32m    541\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 542\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfuture\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    543\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mCfTimeoutError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    544\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mTimeoutError\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\MLenv\\lib\\concurrent\\futures\\_base.py\u001B[0m in \u001B[0;36mresult\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    428\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__get_result\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    429\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 430\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_condition\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    431\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    432\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_state\u001B[0m \u001B[1;32min\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mCANCELLED\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mCANCELLED_AND_NOTIFIED\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\MLenv\\lib\\threading.py\u001B[0m in \u001B[0;36mwait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    294\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m    \u001B[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    295\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 296\u001B[1;33m                 \u001B[0mwaiter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0macquire\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    297\u001B[0m                 \u001B[0mgotit\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    298\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# BEGIN CODE HERE\n",
    "\n",
    "cls1 = DecisionTreeClassifier(random_state = RANDOM_STATE) # Classifier #1\n",
    "cls2 = LogisticRegression(random_state = RANDOM_STATE, n_jobs=-1) # Classifier #2\n",
    "cls3 = KNeighborsClassifier(n_neighbors=3, n_jobs=-1) # Classifier #1\n",
    "soft_vcls = VotingClassifier(estimators=[('DTC', cls1), ('LR', cls2), ('KNN', cls3)], voting='soft') # Voting Classifier\n",
    "hard_vcls = VotingClassifier(estimators=[('DTC', cls1), ('LR', cls2), ('KNN', cls3)], voting='hard') # Voting Classifier\n",
    "\n",
    "svlcs_scores = cross_validate(soft_vcls, X, y, scoring = ['f1_weighted', 'balanced_accuracy'], cv=10, n_jobs=-1)\n",
    "s_avg_fmeasure = sum(svlcs_scores['test_f1_weighted'])/len(svlcs_scores['test_f1_weighted']) # The average f-measure\n",
    "s_avg_accuracy = sum(svlcs_scores['test_balanced_accuracy'])/len(svlcs_scores['test_balanced_accuracy']) # The average accuracy\n",
    "\n",
    "hvlcs_scores = cross_validate(hard_vcls, X, y, scoring = ['f1_weighted', 'balanced_accuracy'], cv=10, n_jobs=-1)\n",
    "h_avg_fmeasure = sum(hvlcs_scores['test_f1_weighted'])/len(hvlcs_scores['test_f1_weighted']) # The average f-measure\n",
    "h_avg_accuracy = sum(hvlcs_scores['test_balanced_accuracy'])/len(hvlcs_scores['test_balanced_accuracy']) # The average accuracy\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "sQQvClrmXMmQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:\n",
      "VotingClassifier(estimators=[('DTC', DecisionTreeClassifier(random_state=42)),\n",
      "                             ('LR',\n",
      "                              LogisticRegression(n_jobs=-1, random_state=42)),\n",
      "                             ('KNN',\n",
      "                              KNeighborsClassifier(n_jobs=-1, n_neighbors=3))],\n",
      "                 voting='soft')\n",
      "F1 Weighted-Score: 0.8232 & Balanced Accuracy: 0.816\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "print(soft_vcls)\n",
    "print(\"F1 Weighted-Score: {} & Balanced Accuracy: {}\".format(round(s_avg_fmeasure,4), round(s_avg_accuracy,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-iJK9pFaDka",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should achive above 82% (Soft Voting Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "XRNkVAvEYVbn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:\n",
      "VotingClassifier(estimators=[('DTC', DecisionTreeClassifier(random_state=42)),\n",
      "                             ('LR',\n",
      "                              LogisticRegression(n_jobs=-1, random_state=42)),\n",
      "                             ('KNN',\n",
      "                              KNeighborsClassifier(n_jobs=-1, n_neighbors=3))])\n",
      "F1 Weighted-Score: 0.8261 & Balanced Accuracy: 0.8191\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "print(hard_vcls)\n",
    "print(\"F1 Weighted-Score: {} & Balanced Accuracy: {}\".format(round(h_avg_fmeasure,4), round(h_avg_accuracy,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6M0CZO6aEHi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should achieve above 80% in both! (Hard Voting Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVPuIxwFXMmR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2 Stacking ###\n",
    "Create a stacking classifier which uses two more complex estimators. Try different simple classifiers (like the ones mentioned before) for the combination of the initial estimators. Report your results in the following cell.\n",
    "\n",
    "Consider as complex estimators the following:\n",
    "\n",
    "*   Random Forest\n",
    "*   SVM\n",
    "*   Gradient Boosting\n",
    "*   MLP\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HX6T1qrFXMmS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# BEGIN CODE HERE\n",
    "cls1 = MLPClassifier(random_state=RANDOM_STATE, max_iter=100) # Classifier #1\n",
    "cls2 = SVC(gamma='auto',random_state=RANDOM_STATE) # Classifier #2\n",
    "cls3 = \"\" # Classifier #3 (Optional)\n",
    "scls = StackingClassifier(estimators=[('SVC',cls1),('MLP',cls2)],final_estimator=LogisticRegression(random_state=RANDOM_STATE,n_jobs=-1),n_jobs=-1) # Stacking Classifier\n",
    "scores = cross_validate(scls,X,y,scoring = ['f1_weighted', 'balanced_accuracy'], cv=10, n_jobs=-1)\n",
    "avg_fmeasure = sum(scores['test_f1_weighted'])/len(scores['test_f1_weighted']) # The average f-measure\n",
    "avg_accuracy = sum(scores['test_balanced_accuracy'])/len(scores['test_balanced_accuracy']) # The average accuracy\n",
    "# Παραθέτω το output, γιατί παίρνει λίγο χρόνο για να τρέξει\n",
    "# F1 Weighted Score: 0.865 & Balanced Accuracy: 0.8589\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:\n",
      "StackingClassifier(estimators=[('SVC',\n",
      "                                MLPClassifier(max_iter=100, random_state=42)),\n",
      "                               ('MLP', SVC(gamma='auto', random_state=42))],\n",
      "                   final_estimator=LogisticRegression(n_jobs=-1,\n",
      "                                                      random_state=42),\n",
      "                   n_jobs=-1)\n",
      "F1 Weighted Score: 0.865 & Balanced Accuracy: 0.8589\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "print(scls)\n",
    "print(\"F1 Weighted Score: {} & Balanced Accuracy: {}\".format(round(avg_fmeasure,4), round(avg_accuracy,4)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcgOx-HPvBI-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should achieve above 85% in both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-nqW51xXMmU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.0 Randomization ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPG8MdFLXMmV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**2.1** You are asked to create three ensembles of decision trees where each one uses a different method for producing homogeneous ensembles. Compare them with a simple decision tree classifier and report your results in the dictionaries (dict) below using as key the given name of your classifier and as value the f1_weighted/balanced_accuracy score. The dictionaries should contain four different elements.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "PmkaP-DjXMmV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "ens1 = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=RANDOM_STATE),n_estimators=200, n_jobs=-1,random_state=RANDOM_STATE)\n",
    "ens2 = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE,n_jobs=-1)\n",
    "ens3 = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=6,min_samples_leaf=5),n_estimators=100,random_state=RANDOM_STATE)\n",
    "tree = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "ens1_scores = cross_validate(ens1,X,y,scoring = ['f1_weighted', 'balanced_accuracy'], cv=3, n_jobs=-1)\n",
    "ens2_scores = cross_validate(ens2,X,y,scoring = ['f1_weighted', 'balanced_accuracy'], cv=3, n_jobs=-1)\n",
    "ens3_scores = cross_validate(ens3,X,y,scoring = ['f1_weighted', 'balanced_accuracy'], cv=3, n_jobs=-1)\n",
    "tree_scores = cross_validate(tree,X,y,scoring = ['f1_weighted', 'balanced_accuracy'], cv=3, n_jobs=-1)\n",
    "\n",
    "f_measures = dict(Bagging = sum(ens1_scores['test_f1_weighted'])/len(ens1_scores['test_f1_weighted']), RandomForest = sum(ens2_scores['test_f1_weighted'])/len(ens2_scores['test_f1_weighted']),AdaBoost = sum(ens3_scores['test_f1_weighted'])/len(ens3_scores['test_f1_weighted']), Simple_DT = sum(tree_scores['test_f1_weighted'])/len(tree_scores['test_f1_weighted']))\n",
    "accuracies = dict(Bagging = sum(ens1_scores['test_balanced_accuracy'])/len(ens1_scores['test_balanced_accuracy']), RandomForest = sum(ens2_scores['test_balanced_accuracy'])/len(ens2_scores['test_balanced_accuracy']),AdaBoost = sum(ens3_scores['test_balanced_accuracy'])/len(ens3_scores['test_balanced_accuracy']), Simple_DT = sum(tree_scores['test_balanced_accuracy'])/len(tree_scores['test_balanced_accuracy']))\n",
    "# Example f_measures = {'Simple Decision': 0.8551, 'Ensemble with random ...': 0.92, ...}\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IUqhDUuCXMmW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=42),\n",
      "                  n_estimators=200, n_jobs=-1, random_state=42)\n",
      "RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=0)\n",
      "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=6,\n",
      "                                                         min_samples_leaf=5,\n",
      "                                                         random_state=42),\n",
      "                   n_estimators=100, random_state=42)\n",
      "DecisionTreeClassifier(random_state=42)\n",
      "Classifier:Bagging -  F1 Weighted:0.8056\n",
      "Classifier:RandomForest -  F1 Weighted:0.7996\n",
      "Classifier:AdaBoost -  F1 Weighted:0.797\n",
      "Classifier:Simple_DT -  F1 Weighted:0.6973\n",
      "Classifier:Bagging -  BalancedAccuracy:0.7926\n",
      "Classifier:RandomForest -  BalancedAccuracy:0.784\n",
      "Classifier:AdaBoost -  BalancedAccuracy:0.7841\n",
      "Classifier:Simple_DT -  BalancedAccuracy:0.6896\n"
     ]
    }
   ],
   "source": [
    "print(ens1)\n",
    "print(ens2)\n",
    "print(ens3)\n",
    "print(tree)\n",
    "for name,score in f_measures.items():\n",
    "    print(\"Classifier:{} -  F1 Weighted:{}\".format(name,round(score,4)))\n",
    "for name,score in accuracies.items():\n",
    "    print(\"Classifier:{} -  BalancedAccuracy:{}\".format(name,round(score,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqdXTE_2XMmX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**2.2** Describe your classifiers and your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rU9POFftXMmX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<ul>\n",
    "    <li>Ο πρώτος classifier που χρησιμοποιήσαμε παράχθηκε με τη μέθοδο του Bagging όπου η παραγωγή των μοντέλων του ensemble επιτυγχάνεται μέσω της εφαρμογής του ίδιου αλγορίθμου (εδώ χρησιμοποιήσαμε Decision Tree Classifier) σε <b>διαφορετικές παραλλαγές του συνόλου εκπαίδευσης</b>, οι οποίες προκύπτουν χρησιμοποιώντας δειγματοληψία με επανατοποθέτηση (η διαδικασία αυτή έγινε 200 φορές λόγω του n_estimators=200). Τα αποτελέσματα και με τις δυο μετρικές είναι κοντά στο 0.8</li>\n",
    "    <li>Ο δεύτερος classifier χρησιμοποιεί την μέθοδο του Random Forest, όπου σε κάθε κόμβο επιλέγεται το καλύτερο ζεύγος μεταβλητής εισόδου και τιμής μεταξύ όχι όλων των διαθέσιμων αλλά ενός τυχαία επιλεγμένου υποσυνόλου των features. Να τονίσουμε ότι χρησιμοποίηθηκαν, όπως και παραπάνω, 200 estimators. Τα αποτελέσματα και με τις δυο μετρικές είναι και πάλι κοντά στο 0.8</li>\n",
    "    <li>Ο τρίτος classifier ακολουθεί την τεχνική του boosting, και πιο συγκεκριμένα χρησιμοποιήσαμε την προσαρμοστική ενίσχυση (AdaBoost).Στην συγκεκριμένη περίπτωση χρησιμοποιήσαμε ως base estimator ένα Decision Tree(max_depth=6,min_samples_leaf=5). Να τονίσουμε ότι χρησιμοποίηθηκαν 100 estimators, που σημαίνει ότι σε κάθε μία από αυτές τις 100 επαναλήψεις προσπαθούμε να κάνουμε το μοντέλο να διορθώσει τα λάθη των προηγούμενων μοντέλων, δίνοντας μεγαλύτερη προσοχή στα παραδείγματα εκπαίδευσης όπου τα προηγούμενα μοντέλα εμφανίζουν την μργαλύτερη απόκλιση από την μεταβλητή στόχο. Το απότελεσμα του F1 σε αυτή την περίπτωση είναι 0.79 και 0.78 στο balanced accuracy.</li>\n",
    "    <li>Αναφορικά με τον τρόπο σύγκρισης των 3 ensembles και του απλού Decision Tree Classifier, να τονίσουμε ότι κάναμε cross validation σε κάθε έναν από τους classifiers και στο τέλος πήραμε των Μ.Ο. των scores του καθενός έτσι ώστε να μπορέσουμε να συγκρίνουμε τα results των διαφορετικών classifiers</li>\n",
    "    <li>Συγκεντρωτικά, παρατηρούμε πώς και τα 3 ensembles πετυχαίνουν περίπου 0.8 και στις 2 μετρικές, ενώ ο απλός Decision Classifier πετυχαίνει μικρότερο score γύρω στο 0.7 και στις 2 μετρικές, το οποίο είναι και αναμενόμενο μιας και είδαμε στην θεωρία τους λόγους για τους οποίους τα ensembles ξεπερενούν (outperform) τα single models</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkJeuV1FXMmX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**2.3** Increasing the number of estimators in a bagging classifier can drastically increase the training time of a classifier. Is there any solution to this problem? Can the same solution be applied to boosting classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApNEPcWEXMmY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<p>Ναι, υπάρχει λύση στο συγκεκριμένο πρόβλημα και αύτη είναι η παραλληλοποίηση της διαδικασίας του bagging. Πιο συγκεκριμένα, στο bagging τα μοντέλα του ensemble μπορούν να εκπαιδευτοούν ανεξάρτητα το ένα από το άλλο και συνεπώς θα μπορούσε να γίνει χρήση πολλαπλών υπολογιστικών πόρων για την επιτάχυνση των διαδικασιών εκπαίδευσης. Π.χ. κάθε ένα από τα threads της cpu θα μπορούσε να εκπαιδεύει και ένα μοντέλο του ensemble. Παραπάνω κάναμε χρήση αυτής της λύσης ώστε να μειώσουμε το trainning time του classifier δίνοντας ως παράμετρο στο ensemble το n_jobs=-1. Ωστόσο, μια τέτοια λύση <b>δεν</b> μπορεί να εφαρμοστεί στην περίπτωση ενός boosting classifier (όπως π.χ. του AdaBoost που χρησιμοποιήσαμε παραπάνω). Αυτό συμβαίνει επειδή τα διαφορετικά μοντέλα παράγονται <b>ακολουθιακά</b>,καθώς κάθε μοντέλο παίρνει είσοδο από το προηγούμενο μοντέλο και αυτή η διαδικασία δεν μπορεί να παραλληλοποιηθεί.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgvsCbUGXMmY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.0 Creating the best classifier ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6daX2mRXMmZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**3.1** In this part of the assignment you are asked to train the best possible ensemble! Describe the process you followed to achieve this result. How did you choose your classifier and your parameters and why. Report the f-measure (weighted) & balanced accuracy (10-fold cross validation) of your final classifier and results of classifiers you tried in the cell following the code. Can you achieve a balanced accuracy over 83-84%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "00xAQ0HfXMmZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "cls1 = MLPClassifier(random_state=RANDOM_STATE, max_iter=100,alpha=0.0001, hidden_layer_sizes=(100,),learning_rate='adaptive')\n",
    "cls2 = AdaBoostClassifier(base_estimator=LogisticRegression(random_state=RANDOM_STATE,n_jobs=-1) ,n_estimators=10, random_state=RANDOM_STATE)\n",
    "cls3 = SVC(gamma='auto',random_state=RANDOM_STATE)\n",
    "best_cls = StackingClassifier(estimators=[('MLP',cls1),('AdaBoost',cls2),('SVC',cls3)],final_estimator=LogisticRegression(random_state=RANDOM_STATE,n_jobs=-1),n_jobs=-1)\n",
    "\n",
    "scores = cross_validate(best_cls,X,y,scoring = ['f1_weighted', 'balanced_accuracy'], cv=10, n_jobs=-1)\n",
    "\n",
    "best_fmeasure = sum(scores['test_f1_weighted'])/len(scores['test_f1_weighted'])\n",
    "best_accuracy = sum(scores['test_balanced_accuracy'])/len(scores['test_balanced_accuracy'])\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FbLB09agXMma",
    "outputId": "be9abfc9-aa0d-4e22-9f07-46926fd1d987",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:\n",
      "F1 Weighted-Score:0.8652637336589921 & Balanced Accuracy:0.8593066509423612\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "#print(best_cls)\n",
    "print(\"F1 Weighted-Score:{} & Balanced Accuracy:{}\".format(best_fmeasure, best_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnos1uqzXMma",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**3.2** Describe the process you followed to achieve this result. How did you choose your classifier and your parameters and why. Report the f-measure & accuracy (10-fold cross validation) of your final classifier and results of classifiers you tried in the cell following the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5dAfbTfXMmb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<ul>\n",
    "    <li>Describe the process you followed to achieve this result\n",
    "        <ol>\n",
    "            <li>Στην προσπάθεια να δημιουργήσουμε τον \"best classifier\" θα χρησιμοποιήσουμε την τεχνική του stacking, με την οποία θα συνδυάσουμε 3 μοντέλα.Το μέτα-μοντέλο που θα χρησιμοποιηθεί στο stacking είναι ένας απλός Logistic Regression.</li>\n",
    "            <li>1ο μοντέλο:\n",
    "            Αρχικά, κάναμε ένα GridSearchCV για να βρούμε το καλύτερο set παραμέτρων για τον MLP Classifier, ο οποίος είναι και ο πρώτος base estimator που χρησιμοποιήσαμε.\n",
    "            Παραθέτω εδώ το αποτέλεσμα του GridSearchCV: Best parameters found: {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive'}</li>\n",
    "            <li>2ο μοντέλο:\n",
    "            Σε αυτήν την περίπτωση χρησιμοποιήσαμε τη μέθοδο του boosting, έχοντας ως base estimator έναν Logistic Regression. Γνωρίζουμε από τη θεωρία ότι όσους πιο πολλούς estimators χρησιμοποιούμε τόσο καλύτερα αποτελέσματα βλέπουμε, αλλά για να μην αυξηθεί επιπλέον το υπολογιστικό κόστος προτιμήσαμε να δώσουμε ως παράμετρο n_estimators=10.</li>\n",
    "            <li>3ο μοντέλο:\n",
    "            Σε αυτήν την περίπτωση προτιμήθηκε ως base estimator ένα SVC με παράμετρο gamma='auto'</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>How did you choose your classifier and your parameters and why\n",
    "        <ol>\n",
    "            <li>Δοκιμάσαμε 8 διαφορετικούς base estimators, από τους οποίους επιλέξαμε εκείνους τους τρεις που πέτυχαν τα καλύτερα αποτελέσματα MLP, AdaBoost, SVM) και τους δώσαμε ως είσοδο στον Stacking Classifier. Στην διαδικασία αυτή παρατηρήσαμε ότι το Random Forest και γενικά τα δενδρικά μοντέλα δεν ήταν ιδιαίτερα αποδοτικά σε αυτό το Data Set, ενώ αποφύγαμε τις τεχνικές του Bagging και του Rnadom Patches μιας και είναι ιδιαίτερα χρονοβόρες.</li>\n",
    "            <li>Για να καταφέρει ένα ensemble να έχει μεγαλύτερη ορθότητα από τα επιμέρους μοντέλα, από τα οποία αποτελείται, θα πρέπει να περιλαμβάνει μοντέλα ικανοποιητικής ορθότητας και τα οποία να διαφέρουν μεταξύ τους ως προς τις περιπτώσεις στις οποίες σφάλλουν.</li>\n",
    "            <li>Για αυτό ακριβώς τον λόγο επλέχθηκαν μοντέλα διαφορετικών οικογενειών (νευρωνικά, γραμμικά), όπου το κάθε ένα από αυτά τα τρία πετυχαίνει από μόνο του μια αρκετά ικανοποιητική ορθότητα, ενώ εξασφαλίζεται και το diversity. Ο συνδυασμός αυτών μέσω του stacking, το οποίο είναι ένας πιο intelligent τρόπος συγκερασμού μοντέλων, μας οδήγησε στο να πετύχουμε ακόμα καλύτερα αποτελέσματα.</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>Report the f-measure & accuracy (10-fold cross validation) of your final classifier and results of classifiers you tried in the cell following the code\n",
    "        <ol>\n",
    "            <li>F1 Weighted-Score: 0.8652637336589921</li>\n",
    "            <li>Balanced Accuracy: 0.8593066509423612</li>\n",
    "            <li>Παράκάτω έχουμε προσθέσει 2 cells, όπου στο πρώτο cell φαίνεται το Grid Search CV που κάναμε, ενώ στο δεύτερο cell φαίνεται ο πειραματισμός που κάναμε προκειμένου να καταλήξουμε στους καλύτερους base estimators και τα διάφορα results των base estimators που δοκιμάσαμε.</li>\n",
    "            <li>results of classifiers you tried:\n",
    "                <br>\n",
    "                Gradient Boosting:<br>\n",
    "                F1-Score: 0.8246217930362637\n",
    "                Accuracy: 0.8151808296208805\n",
    "                <br>\n",
    "                SVC:<br>\n",
    "                F1-Score: 0.8468680264642948\n",
    "                Accuracy: 0.8392989348362883\n",
    "                <br>\n",
    "                MLP:<br>\n",
    "                F1-Score: 0.851622807347304\n",
    "                Accuracy: 0.8458722231494439\n",
    "                <br>\n",
    "                Bagging:<br>\n",
    "                F1-Score: 0.8409584742455878\n",
    "                Accuracy: 0.8358229234870953\n",
    "                <br>\n",
    "                Random Forest:<br>\n",
    "                F1-Score: 0.8066134353605884\n",
    "                Accuracy: 0.7929703046565072\n",
    "                <br>\n",
    "                Random Patches:<br>\n",
    "                F1-Score: 0.8399049624791819\n",
    "                Accuracy: 0.8333087262970047\n",
    "                <br>\n",
    "                AdaBoost:<br>\n",
    "                F1-Score: 0.8526366007300631\n",
    "                Accuracy: 0.8483864203395343\n",
    "                <br>\n",
    "                LogisticRegression:<br>\n",
    "                F1-Score: 0.8329846976316241\n",
    "                Accuracy: 0.8275549592807042\n",
    "            </li>\n",
    "        <ol>\n",
    "    </li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive'}\n"
     ]
    }
   ],
   "source": [
    "# Grid Search για τον MLP Classifier\n",
    "mlp = MLPClassifier(max_iter=100)\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(X, y)\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "# Παρακάτω παραθέτω το output, μιας και είναι χρονοβόρα διαδικασία:\n",
    "\n",
    "# GridSearchCV(cv=5, estimator=MLPClassifier(max_iter=100), n_jobs=-1,\n",
    "#              param_grid={'alpha': [0.0001, 0.05],\n",
    "#                          'hidden_layer_sizes': [(50, 50, 50), (50, 100, 50),\n",
    "#                                                 (100,)],\n",
    "#                          'learning_rate': ['constant', 'adaptive']})\n",
    "\n",
    "# Best parameters found:\n",
    "# {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive'}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting:\n",
      "F1-Score: 0.8246217930362637\n",
      "Accuracy: 0.8151808296208805\n",
      "SVC\n",
      "F1-Score: 0.8468680264642948\n",
      "Accuracy: 0.8392989348362883\n",
      "MLP\n",
      "F1-Score: 0.851622807347304\n",
      "Accuracy: 0.8458722231494439\n",
      "Bagging\n",
      "F1-Score: 0.8409584742455878\n",
      "Accuracy: 0.8358229234870953\n",
      "Random Forest\n",
      "F1-Score: 0.8066134353605884\n",
      "Accuracy: 0.7929703046565072\n",
      "Random Patches\n",
      "F1-Score: 0.8399049624791819\n",
      "Accuracy: 0.8333087262970047\n",
      "AdaBoost\n",
      "F1-Score: 0.8526366007300631\n",
      "Accuracy: 0.8483864203395343\n",
      "LogisticRegression\n",
      "F1-Score: 0.8329846976316241\n",
      "Accuracy: 0.8275549592807042\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#train - test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=RANDOM_STATE)\n",
    "\n",
    "print(\"Gradient Boosting:\")\n",
    "xgb = XGBClassifier(n_estimators=100,use_label_encoder=False, eval_metric='error')\n",
    "y_pred = xgb.fit(X_train,y_train).predict(X_test)\n",
    "print(\"F1-Score: \" + str(f1_score(y_test,y_pred,average='weighted')))\n",
    "print(\"Accuracy: \" + str(balanced_accuracy_score(y_test,y_pred)))\n",
    "\n",
    "print(\"SVC:\")\n",
    "svc = SVC(gamma='auto',random_state=RANDOM_STATE)\n",
    "y_pred = svc.fit(X_train,y_train).predict(X_test)\n",
    "print(\"F1-Score: \" + str(f1_score(y_test,y_pred,average='weighted')))\n",
    "print(\"Accuracy: \" + str(balanced_accuracy_score(y_test,y_pred)))\n",
    "\n",
    "\n",
    "print(\"MLP:\")\n",
    "mlp = MLPClassifier(random_state=RANDOM_STATE, max_iter=100,alpha=0.0001, hidden_layer_sizes=(100,),learning_rate='adaptive')\n",
    "y_pred = mlp.fit(X_train,y_train).predict(X_test)\n",
    "print(\"F1-Score: \" + str(f1_score(y_test,y_pred,average='weighted')))\n",
    "print(\"Accuracy: \" + str(balanced_accuracy_score(y_test,y_pred)))\n",
    "\n",
    "\n",
    "print(\"Bagging:\")\n",
    "bagging = BaggingClassifier(LogisticRegression(random_state=RANDOM_STATE, n_jobs=-1), n_estimators=50, n_jobs=-1, random_state=RANDOM_STATE)\n",
    "y_pred = bagging.fit(X_train,y_train).predict(X_test)\n",
    "print(\"F1-Score: \" + str(f1_score(y_test,y_pred,average='weighted')))\n",
    "print(\"Accuracy: \" + str(balanced_accuracy_score(y_test,y_pred)))\n",
    "\n",
    "print(\"Random Forest:\")\n",
    "rfc = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE,n_jobs=-1)\n",
    "y_pred = rfc.fit(X_train,y_train).predict(X_test)\n",
    "print(\"F1-Score: \" + str(f1_score(y_test,y_pred,average='weighted')))\n",
    "print(\"Accuracy: \" + str(balanced_accuracy_score(y_test,y_pred)))\n",
    "BaggingClassifier(base_estimator=SVC(gamma='auto',random_state=RANDOM_STATE),n_estimators=10, random_state=RANDOM_STATE, max_features=0.7, max_samples=0.7)\n",
    "\n",
    "print(\"Random Patches:\")\n",
    "rp = BaggingClassifier(base_estimator=LogisticRegression(random_state=RANDOM_STATE, n_jobs=-1),n_estimators=10, random_state=RANDOM_STATE, max_features=0.7, max_samples=0.7)\n",
    "y_pred = rp.fit(X_train,y_train).predict(X_test)\n",
    "print(\"F1-Score: \" + str(f1_score(y_test,y_pred,average='weighted')))\n",
    "print(\"Accuracy: \" + str(balanced_accuracy_score(y_test,y_pred)))\n",
    "\n",
    "print(\"AdaBoost:\")\n",
    "ada = AdaBoostClassifier(base_estimator=LogisticRegression(random_state=RANDOM_STATE,n_jobs=-1) ,n_estimators=10, random_state=RANDOM_STATE)\n",
    "y_pred = ada.fit(X_train,y_train).predict(X_test)\n",
    "print(\"F1-Score: \" + str(f1_score(y_test,y_pred,average='weighted')))\n",
    "print(\"Accuracy: \" + str(balanced_accuracy_score(y_test,y_pred)))\n",
    "\n",
    "print(\"LogisticRegression:\")\n",
    "lr = LogisticRegression(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "y_pred = lr.fit(X_train,y_train).predict(X_test)\n",
    "print(\"F1-Score: \" + str(f1_score(y_test,y_pred,average='weighted')))\n",
    "print(\"Accuracy: \" + str(balanced_accuracy_score(y_test,y_pred)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQEFCmbcXMmb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**3.3** Create a classifier that is going to be used in production - in a live system. Use the *test_set_noclass.csv* to make predictions. Store the predictions in a list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "XQPgm_ubXMmc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "cls1 = MLPClassifier(random_state=RANDOM_STATE, max_iter=100,alpha=0.0001, hidden_layer_sizes=(100,),learning_rate='adaptive')\n",
    "cls2 = AdaBoostClassifier(base_estimator=LogisticRegression(random_state=RANDOM_STATE,n_jobs=-1) ,n_estimators=10, random_state=RANDOM_STATE)\n",
    "cls3 = SVC(gamma='auto',random_state=RANDOM_STATE)\n",
    "best_cls = StackingClassifier(estimators=[('MLP',cls1),('AdaBoost',cls2),('SVC',cls3)],final_estimator=LogisticRegression(random_state=RANDOM_STATE,n_jobs=-1),n_jobs=-1)\n",
    "\n",
    "cls = best_cls.fit(X,y)\n",
    "#END CODE HERE\n",
    "test_set = pd.read_csv(\"test_set_noclass.csv\")\n",
    "predictions = cls.predict(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnAp-d2DXMmf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<p>Ο τελικός classifier που φτιάξαμε προσπαθήσαμε να είναι αρκετά δημιουργικός μιας και χρησιμοποιεί μία από τις τεχνικές παραγωγής homogenous ensemble όπως το boosting (AdaBoost) ενώ ως base estimator χρησιμοποιούμε γραμμικά μοντέλα (Logistic Regression & SVC) και νευρωνικά μοντέλα (Multi-layer Perceptron classifier). Δηλαδή, υπάρχει μια ποικιλία αλγορίθμων και τεχνικών. Μάλιστα, ο συγκερασμός των επιμέρους classifiers (cls1,cls2,cls3) γίνεται με την τεχνική του stacking έχοντας ως μέτα-μοντέλο έναν Logistic Regression.</p>\n",
    "<p>Υπολογίσαμε παραπάνω τον Μ.Ο. της μετρικής f1(weighted) και της μετρικής balanced accuracy, οπώς αυτός προέκυψε ύστερα από την διαδικασία του 10 fold CV. Θεωρούμε πως αυτή θα είναι και η ακρίβεια του μοντέλου στις 2 αυτές μετρικές γενικότερα και για αυτό ακριβώς τον λόγο εκπαιδεύσαμε τον τελικό classifier με όλα τα διαθέσιμα data που έχουμε και ο classifier είναι έτοιμος να χρησιμοποιηθεί στο production.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Neagvu0TXMmg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### This following cell will not be executed. The test_set.csv with the classes will be made available after the deadline and this cell is for testing purposes!!! Do not modify it! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7K7iI7BXMmg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "  final_test_set = pd.read_csv('test_set.csv')\n",
    "  ground_truth = final_test_set['CLASS']\n",
    "  print(\"Balanced Accuracy: {}\".format(balanced_accuracy_score(predictions, ground_truth)))\n",
    "  print(\"F1 Weighted-Score: {}\".format(f1_score(predictions, ground_truth, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Both should aim above 85%!"
   ],
   "metadata": {
    "id": "YJH-9KdOzW7z",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "EnsembleMethods.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}